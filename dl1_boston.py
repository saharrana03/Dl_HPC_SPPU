# -*- coding: utf-8 -*-
"""DL_BOSTON.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmttPfFTJ4ouKzbSPcSIhtp1gabs_R9S
"""

# Step 0: Install required libraries in Google Colab
# Run this cell in Colab to ensure all dependencies are installed
try:
    import tensorflow
except ImportError:
    !pip install tensorflow
try:
    import sklearn
except ImportError:
    !pip install scikit-learn
try:
    import matplotlib
except ImportError:
    !pip install matplotlib

# Importing necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import os

# Step 1: Loading the dataset
# Instructions:
# - In Google Colab: Upload 'housing.csv' via the Files tab (left sidebar)
# - In Jupyter: Ensure 'housing.csv' is in the same directory as this notebook
# Define column names for the Boston Housing dataset
column_names = [
    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',
    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'
]

try:
    # Reading the CSV file
    # Using delim_whitespace=True for space-separated data
    df = pd.read_csv(
        '/content/housing.csv',
        delim_whitespace=True,
        names=column_names,
        skipinitialspace=True,
        skiprows=0
    )
    print("Dataset loaded successfully!")
    print("First few rows of the dataset:")
    print(df.head())
except FileNotFoundError:
    print("Error: 'housing.csv' not found. Please upload the file in Colab or place it in the Jupyter directory.")
    raise
except Exception as e:
    print(f"Error loading dataset: {str(e)}")
    raise

# Step 2: Data Preprocessing
try:
    # Checking for missing values
    print("\nChecking for missing values:")
    print(df.isnull().sum())

    # Separating features (X) and target (y)
    X = df.drop('MEDV', axis=1)
    y = df['MEDV']

    # Splitting the data into training and testing sets (80-20 split)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Standardizing the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("\nData preprocessing completed successfully!")
except Exception as e:
    print(f"Error in preprocessing: {str(e)}")
    raise

# Step 3: Building the Deep Neural Network
try:
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    print("\nNeural network model built successfully!")
except Exception as e:
    print(f"Error building model: {str(e)}")
    raise

# Step 4: Compiling the model
try:
    model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )
    print("\nModel compiled successfully!")
except Exception as e:
    print(f"Error compiling model: {str(e)}")
    raise

# Step 5: Training the model
try:
    history = model.fit(
        X_train_scaled, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        verbose=1
    )
    print("\nModel training completed successfully!")
except Exception as e:
    print(f"Error training model: {str(e)}")
    raise

# Step 6: Evaluating the model
try:
    test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
    print(f"\nTest Mean Absolute Error: ${test_mae:.2f}K")
    print(f"Test Mean Squared Error: {test_loss:.2f}")
except Exception as e:
    print(f"Error evaluating model: {str(e)}")
    raise

# Step 7: Making predictions
try:
    y_pred = model.predict(X_test_scaled)
    print("\nPredictions generated successfully!")
except Exception as e:
    print(f"Error making predictions: {str(e)}")
    raise

# Step 8: Visualizing the training process
try:
    plt.figure(figsize=(12, 5))

    # Plotting loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.grid(True)

    # Plotting MAE
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.title('Model MAE')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error ($K)')
    plt.legend()
    plt.grid(True)

    # Display the plot in the notebook
    plt.show()

    # Saving the plot
    plt.savefig('training_metrics.png')
    plt.close()
    print("\nTraining metrics plot displayed and saved as 'training_metrics.png'")
except Exception as e:
    print(f"Error visualizing training metrics: {str(e)}")
    raise

# Step 9: Visualizing predictions vs actual values
try:
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Price ($K)')
    plt.ylabel('Predicted Price ($K)')
    plt.title('Actual vs Predicted House Prices')
    plt.grid(True)

    # Display the plot in the notebook
    plt.show()

    # Saving the scatter plot
    plt.savefig('predictions_vs_actual.png')
    plt.close()
    print("Predictions vs actual plot displayed and saved as 'predictions_vs_actual.png'")
except Exception as e:
    print(f"Error visualizing predictions: {str(e)}")
    raise

# Step 10: Saving the model
try:
    model.save('boston_housing_dnn.h5')
    print("\nModel saved as 'boston_housing_dnn.h5'")
except Exception as e:
    print(f"Error saving model: {str(e)}")
    raise

print("\nAll processes completed successfully!")

